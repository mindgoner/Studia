{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8040cf47-8710-4db7-8e73-c8f571954fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 89.8401\n",
      "Epoch 2/10, Loss: 3.9460\n",
      "Epoch 3/10, Loss: 0.5755\n",
      "Epoch 4/10, Loss: 0.2469\n",
      "Epoch 5/10, Loss: 0.1356\n",
      "Epoch 6/10, Loss: 0.0856\n",
      "Epoch 7/10, Loss: 0.0589\n",
      "Epoch 8/10, Loss: 0.0429\n",
      "Epoch 9/10, Loss: 0.0326\n",
      "Epoch 10/10, Loss: 0.0255\n",
      "\n",
      "Testowanie modelu:\n",
      "123456 + 654321 = 777777 (model), 777777 (true)\n",
      "1000000 + 2000000 = 3000000 (model), 3000000 (true)\n",
      "0 + 0 = 0 (model), 0 (true)\n",
      "134217728 + 67108864 = 201326592 (model), 201326592 (true)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Parametry\n",
    "BIT_LENGTH = 28\n",
    "SUM_BIT_LENGTH = BIT_LENGTH + 1  # max suma dwóch 24-bitowych liczb to 25 bitów\n",
    "\n",
    "# Funkcja konwertująca liczbę całkowitą na wektor bitów (LSB first)\n",
    "def int_to_bin_array(x, length=BIT_LENGTH):\n",
    "    return np.array([int(b) for b in np.binary_repr(x, width=length)][::-1])\n",
    "\n",
    "# Funkcja konwertująca wektor bitów na liczbę całkowitą\n",
    "def bin_array_to_int(arr):\n",
    "    return int(\"\".join(str(b) for b in arr[::-1]), 2)\n",
    "\n",
    "# Generujemy dane treningowe\n",
    "def generate_data(num_samples):\n",
    "    X = []\n",
    "    Y = []\n",
    "    for _ in range(num_samples):\n",
    "        a = np.random.randint(0, 2**BIT_LENGTH)\n",
    "        b = np.random.randint(0, 2**BIT_LENGTH)\n",
    "        a_bin = int_to_bin_array(a)\n",
    "        b_bin = int_to_bin_array(b)\n",
    "        s_bin = int_to_bin_array(a + b, length=SUM_BIT_LENGTH)  # suma 25-bit\n",
    "\n",
    "        # Dodajemy krok czasowy z zerami do wejścia, żeby mieć długość 25\n",
    "        a_bin_extended = np.append(a_bin, 0)\n",
    "        b_bin_extended = np.append(b_bin, 0)\n",
    "        \n",
    "        X.append(np.vstack([a_bin_extended, b_bin_extended]).T)  # shape (25, 2)\n",
    "        Y.append(s_bin)  # shape (25,)\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "\n",
    "# Model RNN\n",
    "class BinaryAdderRNN(nn.Module):\n",
    "    def __init__(self, input_size=2, hidden_size=16, output_size=1):\n",
    "        super(BinaryAdderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_size)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out.squeeze(-1)  # shape (batch, seq_len)\n",
    "\n",
    "# Przygotowanie danych do PyTorch\n",
    "def prepare_tensor_data(X, Y):\n",
    "    X_t = torch.tensor(X).float()\n",
    "    Y_t = torch.tensor(Y).float()\n",
    "    return X_t, Y_t\n",
    "\n",
    "# Hyperparametry\n",
    "num_samples = 10000\n",
    "batch_size = 64\n",
    "epochs = 10\n",
    "\n",
    "# Generowanie danych\n",
    "X, Y = generate_data(num_samples)\n",
    "X_t, Y_t = prepare_tensor_data(X, Y)\n",
    "\n",
    "# Model, loss, optimizer\n",
    "model = BinaryAdderRNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Trening\n",
    "for epoch in range(epochs):\n",
    "    permutation = torch.randperm(X_t.size()[0])\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i in range(0, X_t.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        indices = permutation[i:i+batch_size]\n",
    "        batch_x, batch_y = X_t[indices], Y_t[indices]\n",
    "\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Testowanie modelu na kilku przykładach\n",
    "def test_model(model, a, b):\n",
    "    a_bin = int_to_bin_array(a)\n",
    "    b_bin = int_to_bin_array(b)\n",
    "    x = np.vstack([a_bin, b_bin]).T\n",
    "    x_t = torch.tensor(x).unsqueeze(0).float()  # batch 1\n",
    "    with torch.no_grad():\n",
    "        output = model(x_t).round().numpy().astype(int).flatten()\n",
    "    sum_pred = bin_array_to_int(output)\n",
    "    print(f\"{a} + {b} = {sum_pred} (model), {a + b} (true)\")\n",
    "\n",
    "print(\"\\nTestowanie modelu:\")\n",
    "test_model(model, 123456, 654321)\n",
    "test_model(model, 1000000, 2000000)\n",
    "test_model(model, 0, 0)\n",
    "test_model(model, 2**27, 2**26)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964010ab-77be-40b5-9cbd-672f6ab7e9fc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
