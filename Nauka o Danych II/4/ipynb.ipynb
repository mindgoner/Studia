{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93ed2bae-66d2-4e3f-9c7a-38e69e5db623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, SimpleRNN, LayerNormalization, MultiHeadAttention, Dropout\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aecccf9-c8b6-4b2a-a37e-cb57843107c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.2840 - loss: 1.4661 - val_accuracy: 0.2667 - val_loss: 1.1364\n",
      "Epoch 2/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.1866 - loss: 1.0432 - val_accuracy: 0.2333 - val_loss: 0.9912\n",
      "Epoch 3/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.4380 - loss: 0.9586 - val_accuracy: 0.5667 - val_loss: 0.9232\n",
      "Epoch 4/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6630 - loss: 0.9074 - val_accuracy: 0.7000 - val_loss: 0.8845\n",
      "Epoch 5/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6896 - loss: 0.8670 - val_accuracy: 0.7000 - val_loss: 0.8312\n",
      "Epoch 6/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6166 - loss: 0.8606 - val_accuracy: 0.7000 - val_loss: 0.8006\n",
      "Epoch 7/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7014 - loss: 0.8035 - val_accuracy: 0.8000 - val_loss: 0.7732\n",
      "Epoch 8/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7292 - loss: 0.7610 - val_accuracy: 0.7000 - val_loss: 0.7339\n",
      "Epoch 9/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6467 - loss: 0.7459 - val_accuracy: 0.7000 - val_loss: 0.7059\n",
      "Epoch 10/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.6970 - loss: 0.7042 - val_accuracy: 0.8333 - val_loss: 0.6828\n",
      "Epoch 11/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7931 - loss: 0.6760 - val_accuracy: 0.8000 - val_loss: 0.6630\n",
      "Epoch 12/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8871 - loss: 0.6433 - val_accuracy: 0.8667 - val_loss: 0.6461\n",
      "Epoch 13/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8917 - loss: 0.6479 - val_accuracy: 0.8667 - val_loss: 0.6222\n",
      "Epoch 14/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8526 - loss: 0.6407 - val_accuracy: 0.8333 - val_loss: 0.5966\n",
      "Epoch 15/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7812 - loss: 0.6003 - val_accuracy: 0.8333 - val_loss: 0.5763\n",
      "Epoch 16/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8164 - loss: 0.5511 - val_accuracy: 0.8333 - val_loss: 0.5607\n",
      "Epoch 17/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8985 - loss: 0.5715 - val_accuracy: 0.9333 - val_loss: 0.5547\n",
      "Epoch 18/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9292 - loss: 0.5766 - val_accuracy: 0.9000 - val_loss: 0.5330\n",
      "Epoch 19/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8950 - loss: 0.5336 - val_accuracy: 0.8333 - val_loss: 0.5134\n",
      "Epoch 20/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.9171 - loss: 0.5229 - val_accuracy: 0.9000 - val_loss: 0.5010\n",
      "Epoch 21/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9160 - loss: 0.4965 - val_accuracy: 0.9000 - val_loss: 0.4881\n",
      "Epoch 22/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9088 - loss: 0.4892 - val_accuracy: 1.0000 - val_loss: 0.4810\n",
      "Epoch 23/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9475 - loss: 0.4930 - val_accuracy: 0.9667 - val_loss: 0.4672\n",
      "Epoch 24/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9158 - loss: 0.4572 - val_accuracy: 0.8333 - val_loss: 0.4508\n",
      "Epoch 25/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8965 - loss: 0.4764 - val_accuracy: 0.9333 - val_loss: 0.4416\n",
      "Epoch 26/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9292 - loss: 0.4265 - val_accuracy: 1.0000 - val_loss: 0.4348\n",
      "Epoch 27/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9231 - loss: 0.4338 - val_accuracy: 1.0000 - val_loss: 0.4289\n",
      "Epoch 28/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9591 - loss: 0.4327 - val_accuracy: 1.0000 - val_loss: 0.4185\n",
      "Epoch 29/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9392 - loss: 0.4027 - val_accuracy: 0.8333 - val_loss: 0.4017\n",
      "Epoch 30/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8819 - loss: 0.4030 - val_accuracy: 0.9333 - val_loss: 0.3927\n",
      "Epoch 31/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9128 - loss: 0.4012 - val_accuracy: 0.9667 - val_loss: 0.3845\n",
      "Epoch 32/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9635 - loss: 0.3752 - val_accuracy: 1.0000 - val_loss: 0.3806\n",
      "Epoch 33/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9591 - loss: 0.4061 - val_accuracy: 1.0000 - val_loss: 0.3758\n",
      "Epoch 34/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9680 - loss: 0.3712 - val_accuracy: 0.9667 - val_loss: 0.3603\n",
      "Epoch 35/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9372 - loss: 0.3924 - val_accuracy: 0.9667 - val_loss: 0.3534\n",
      "Epoch 36/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9634 - loss: 0.3540 - val_accuracy: 1.0000 - val_loss: 0.3496\n",
      "Epoch 37/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9349 - loss: 0.3755 - val_accuracy: 1.0000 - val_loss: 0.3391\n",
      "Epoch 38/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9644 - loss: 0.3588 - val_accuracy: 1.0000 - val_loss: 0.3335\n",
      "Epoch 39/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9721 - loss: 0.3459 - val_accuracy: 1.0000 - val_loss: 0.3246\n",
      "Epoch 40/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9717 - loss: 0.3492 - val_accuracy: 1.0000 - val_loss: 0.3181\n",
      "Epoch 41/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9837 - loss: 0.3410 - val_accuracy: 1.0000 - val_loss: 0.3113\n",
      "Epoch 42/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9860 - loss: 0.3040 - val_accuracy: 1.0000 - val_loss: 0.3083\n",
      "Epoch 43/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9591 - loss: 0.3319 - val_accuracy: 1.0000 - val_loss: 0.3052\n",
      "Epoch 44/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9602 - loss: 0.3260 - val_accuracy: 1.0000 - val_loss: 0.2923\n",
      "Epoch 45/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9710 - loss: 0.2917 - val_accuracy: 1.0000 - val_loss: 0.2863\n",
      "Epoch 46/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9629 - loss: 0.2966 - val_accuracy: 1.0000 - val_loss: 0.2802\n",
      "Epoch 47/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9866 - loss: 0.3123 - val_accuracy: 1.0000 - val_loss: 0.2763\n",
      "Epoch 48/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9391 - loss: 0.2887 - val_accuracy: 1.0000 - val_loss: 0.2707\n",
      "Epoch 49/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9672 - loss: 0.2821 - val_accuracy: 1.0000 - val_loss: 0.2636\n",
      "Epoch 50/50\n",
      "\u001b[1m8/8\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9860 - loss: 0.2847 - val_accuracy: 1.0000 - val_loss: 0.2585\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - accuracy: 1.0000 - loss: 0.2585\n",
      "Accuracy for IRIS dataset: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Wczytanie zbioru danych IRIS\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import tensorflow as tf\n",
    "\n",
    "# Załadowanie danych IRIS\n",
    "iris = load_iris()\n",
    "X_iris = iris.data\n",
    "y_iris = iris.target\n",
    "\n",
    "# Podział na dane treningowe i testowe\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_iris, y_iris, test_size=0.2, random_state=42)\n",
    "\n",
    "# Model\n",
    "model_iris = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Input(shape=(4,)),  # Użycie Input() zamiast input_dim\n",
    "    tf.keras.layers.Dense(64, activation='softplus'),\n",
    "    tf.keras.layers.Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Kompilacja modelu\n",
    "model_iris.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Trenowanie modelu\n",
    "model_iris.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Ocena modelu\n",
    "loss, accuracy = model_iris.evaluate(X_test, y_test)\n",
    "print(f'Accuracy for IRIS dataset: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb159bb7-b7d8-4782-853e-91577e5d370f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9056 - loss: 0.3274 - val_accuracy: 0.9723 - val_loss: 0.0854\n",
      "Epoch 2/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9799 - loss: 0.0667 - val_accuracy: 0.9809 - val_loss: 0.0646\n",
      "Epoch 3/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9862 - loss: 0.0463 - val_accuracy: 0.9804 - val_loss: 0.0622\n",
      "Epoch 4/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9899 - loss: 0.0343 - val_accuracy: 0.9788 - val_loss: 0.0676\n",
      "Epoch 5/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9922 - loss: 0.0259 - val_accuracy: 0.9822 - val_loss: 0.0634\n",
      "Epoch 6/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9948 - loss: 0.0181 - val_accuracy: 0.9815 - val_loss: 0.0666\n",
      "Epoch 7/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9951 - loss: 0.0159 - val_accuracy: 0.9809 - val_loss: 0.0691\n",
      "Epoch 8/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9971 - loss: 0.0100 - val_accuracy: 0.9794 - val_loss: 0.0764\n",
      "Epoch 9/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9976 - loss: 0.0092 - val_accuracy: 0.9822 - val_loss: 0.0753\n",
      "Epoch 10/10\n",
      "\u001b[1m1875/1875\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.9981 - loss: 0.0066 - val_accuracy: 0.9813 - val_loss: 0.0826\n",
      "\u001b[1m313/313\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 958us/step - accuracy: 0.9768 - loss: 0.1003\n",
      "Accuracy for MNIST dataset: 98.13%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, Input\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Załadowanie zbioru MNIST\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Przekształcenie danych do formatu odpowiedniego dla sieci konwolucyjnej\n",
    "x_train = x_train.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "x_test = x_test.reshape(-1, 28, 28, 1).astype('float32') / 255\n",
    "\n",
    "# Jednohotne zakodowanie etykiet\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Model konwolucyjny\n",
    "model_cnn = tf.keras.models.Sequential([\n",
    "    Input(shape=(28, 28, 1)),  # Użycie Input() zamiast input_shape\n",
    "    Conv2D(32, kernel_size=(3, 3), activation='relu'),\n",
    "    Flatten(),\n",
    "    Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Kompilacja modelu\n",
    "model_cnn.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Trenowanie modelu\n",
    "model_cnn.fit(x_train, y_train, epochs=10, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "# Ocena modelu\n",
    "loss, accuracy = model_cnn.evaluate(x_test, y_test)\n",
    "print(f'Accuracy for MNIST dataset: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "90e91021-5155-483d-8ccd-dcbd659b6a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 18ms/step - accuracy: 0.5814 - loss: 0.7286 - val_accuracy: 0.7285 - val_loss: 0.5396\n",
      "Epoch 2/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 18ms/step - accuracy: 0.7502 - loss: 0.6388 - val_accuracy: 0.8015 - val_loss: 0.4366\n",
      "Epoch 3/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8242 - loss: 0.4163 - val_accuracy: 0.7933 - val_loss: 0.4453\n",
      "Epoch 4/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.8852 - loss: 0.2963 - val_accuracy: 0.8109 - val_loss: 0.4471\n",
      "Epoch 5/5\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 19ms/step - accuracy: 0.9117 - loss: 0.2385 - val_accuracy: 0.7648 - val_loss: 0.5535\n",
      "\u001b[1m782/782\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7609 - loss: 0.5648\n",
      "Accuracy for IMDB sentiment analysis: 76.48%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import SimpleRNN, Dense, Dropout, Embedding, Input\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "# Załadowanie danych IMDB, ograniczając liczbę słów do 20000\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=20000)\n",
    "\n",
    "# Przygotowanie danych (przycinanie sekwencji do 200 słów)\n",
    "max_len = 200  # Maksymalna długość sekwencji\n",
    "x_train = pad_sequences(x_train, maxlen=max_len)\n",
    "x_test = pad_sequences(x_test, maxlen=max_len)\n",
    "\n",
    "# Model RNN\n",
    "model_rnn = Sequential([\n",
    "    Input(shape=(max_len,)),  # Wejście\n",
    "    Embedding(input_dim=20000, output_dim=128),  # Warstwa embedding\n",
    "    SimpleRNN(128, activation='relu'),  # Warstwa RNN\n",
    "    Dropout(0.5),\n",
    "    Dense(1, activation='sigmoid')  # Warstwa wyjściowa z aktywacją sigmoidalną\n",
    "])\n",
    "\n",
    "# Kompilacja modelu\n",
    "model_rnn.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Trenowanie modelu\n",
    "model_rnn.fit(x_train, y_train, epochs=5, batch_size=32, validation_data=(x_test, y_test))\n",
    "\n",
    "# Ocena modelu\n",
    "loss, accuracy = model_rnn.evaluate(x_test, y_test)\n",
    "print(f'Accuracy for IMDB sentiment analysis: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f9e9dc0b-98a5-43ff-b579-747e91d239b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - loss: 2.2391\n",
      "Epoch 2/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.8231 \n",
      "Epoch 3/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1.2647 \n",
      "Epoch 4/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.8212 \n",
      "Epoch 5/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.6206 \n",
      "Epoch 6/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.4570 \n",
      "Epoch 7/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.3637 \n",
      "Epoch 8/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2873 \n",
      "Epoch 9/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.2463 \n",
      "Epoch 10/10\n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.2285 \n",
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
      "Predictions from Transformer model: [[[ 0.51787126]\n",
      "  [ 0.8001105 ]\n",
      "  [ 1.2310444 ]\n",
      "  [ 0.12402429]\n",
      "  [ 1.1900711 ]\n",
      "  [ 0.09617489]\n",
      "  [ 0.04673089]\n",
      "  [ 0.52128917]\n",
      "  [ 0.5379624 ]\n",
      "  [ 0.39469156]]\n",
      "\n",
      " [[ 0.30353978]\n",
      "  [ 0.89934   ]\n",
      "  [ 0.14334758]\n",
      "  [ 0.70972604]\n",
      "  [ 0.6310268 ]\n",
      "  [ 0.80318433]\n",
      "  [ 0.57530934]\n",
      "  [ 0.31162134]\n",
      "  [ 0.61883074]\n",
      "  [ 0.7406059 ]]\n",
      "\n",
      " [[ 0.6182496 ]\n",
      "  [ 0.12775749]\n",
      "  [ 0.6190339 ]\n",
      "  [ 0.21829526]\n",
      "  [ 0.69311154]\n",
      "  [ 0.5239795 ]\n",
      "  [ 0.60166913]\n",
      "  [ 0.5891865 ]\n",
      "  [ 0.5285214 ]\n",
      "  [ 0.32452688]]\n",
      "\n",
      " [[ 0.8504525 ]\n",
      "  [ 0.44511703]\n",
      "  [ 0.5371476 ]\n",
      "  [ 0.9753831 ]\n",
      "  [ 0.36695126]\n",
      "  [-0.54100436]\n",
      "  [ 0.65754807]\n",
      "  [ 0.60469943]\n",
      "  [ 0.20457709]\n",
      "  [ 0.53949475]]\n",
      "\n",
      " [[ 0.15623482]\n",
      "  [ 0.34280673]\n",
      "  [ 1.2441528 ]\n",
      "  [ 0.9151012 ]\n",
      "  [ 0.29407927]\n",
      "  [ 0.36254665]\n",
      "  [ 0.14388983]\n",
      "  [ 0.7265446 ]\n",
      "  [ 0.23881759]\n",
      "  [ 0.05931329]]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import MultiHeadAttention, LayerNormalization, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "# Model Transformer bez pozycyjnego kodowania\n",
    "class SimpleSelfAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, key_dim):\n",
    "        super(SimpleSelfAttention, self).__init__()\n",
    "        self.att = MultiHeadAttention(num_heads=num_heads, key_dim=key_dim)\n",
    "        self.norm = LayerNormalization()\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        return self.norm(attn_output + inputs)\n",
    "\n",
    "# Przygotowanie danych\n",
    "X_transformer = np.random.rand(100, 10, 64)  # 100 próbek, 10 timesteps, 64 cechy\n",
    "y_transformer = np.random.rand(100, 1)  # 100 próbek, 1 wynik\n",
    "\n",
    "# Model\n",
    "inputs = tf.keras.Input(shape=(10, 64))\n",
    "x = SimpleSelfAttention(num_heads=2, key_dim=64)(inputs)\n",
    "x = Dense(1)(x)\n",
    "model_transformer = Model(inputs=inputs, outputs=x)\n",
    "\n",
    "# Kompilacja i trenowanie modelu\n",
    "model_transformer.compile(optimizer='adam', loss='mean_squared_error')\n",
    "model_transformer.fit(X_transformer, y_transformer, epochs=10, batch_size=32)\n",
    "\n",
    "# Predykcja\n",
    "y_pred_transformer = model_transformer.predict(X_transformer)\n",
    "print(f'Predictions from Transformer model: {y_pred_transformer[:5]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
