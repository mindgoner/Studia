{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "a) Import bibliotek"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "Traceback (most recent call last):\n  File \"c:\\Programs\\Python\\3.11\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Procedura inicjowania biblioteki dołączanej dynamicznie (DLL) nie powiodła się.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programs\\Python\\3.11\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:73\u001b[39m\n\u001b[32m     72\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m   \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_pywrap_tensorflow_internal\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m *\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# This try catch logic is because there is no bazel equivalent for py_extension.\u001b[39;00m\n\u001b[32m     75\u001b[39m \u001b[38;5;66;03m# Externally in opensource we must enable exceptions to load the shared object\u001b[39;00m\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# by exposing the PyInit symbols with pybind. This error will only be\u001b[39;00m\n\u001b[32m     77\u001b[39m \u001b[38;5;66;03m# caught internally or if someone changes the name of the target _pywrap_tensorflow_internal.\u001b[39;00m\n\u001b[32m     78\u001b[39m \n\u001b[32m     79\u001b[39m \u001b[38;5;66;03m# This logic is used in other internal projects using py_extension.\u001b[39;00m\n",
            "\u001b[31mImportError\u001b[39m: DLL load failed while importing _pywrap_tensorflow_internal: Procedura inicjowania biblioteki dołączanej dynamicznie (DLL) nie powiodła się.",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrandom\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m layers\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programs\\Python\\3.11\\Lib\\site-packages\\tensorflow\\__init__.py:40\u001b[39m\n\u001b[32m     37\u001b[39m _os.environ.setdefault(\u001b[33m\"\u001b[39m\u001b[33mENABLE_RUNTIME_UPTIME_TELEMETRY\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Do not remove this line; See https://github.com/tensorflow/tensorflow/issues/42596\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m pywrap_tensorflow \u001b[38;5;28;01mas\u001b[39;00m _pywrap_tensorflow  \u001b[38;5;66;03m# pylint: disable=unused-import\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtools\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m module_util \u001b[38;5;28;01mas\u001b[39;00m _module_util\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutil\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlazy_loader\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m KerasLazyLoader \u001b[38;5;28;01mas\u001b[39;00m _KerasLazyLoader\n",
            "\u001b[36mFile \u001b[39m\u001b[32mc:\\Programs\\Python\\3.11\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py:88\u001b[39m\n\u001b[32m     86\u001b[39m     sys.setdlopenflags(_default_dlopen_flags)\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     89\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtraceback.format_exc()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     90\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mFailed to load the native TensorFlow runtime.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     91\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mSee https://www.tensorflow.org/install/errors \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     92\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mfor some common causes and solutions.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     93\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mIf you need help, create an issue \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     94\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mat https://github.com/tensorflow/tensorflow/issues \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m     95\u001b[39m       \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mand include the entire stack trace above this error message.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     97\u001b[39m \u001b[38;5;66;03m# pylint: enable=wildcard-import,g-import-not-at-top,unused-import,line-too-long\u001b[39;00m\n",
            "\u001b[31mImportError\u001b[39m: Traceback (most recent call last):\n  File \"c:\\Programs\\Python\\3.11\\Lib\\site-packages\\tensorflow\\python\\pywrap_tensorflow.py\", line 73, in <module>\n    from tensorflow.python._pywrap_tensorflow_internal import *\nImportError: DLL load failed while importing _pywrap_tensorflow_internal: Procedura inicjowania biblioteki dołączanej dynamicznie (DLL) nie powiodła się.\n\n\nFailed to load the native TensorFlow runtime.\nSee https://www.tensorflow.org/install/errors for some common causes and solutions.\nIf you need help, create an issue at https://github.com/tensorflow/tensorflow/issues and include the entire stack trace above this error message."
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b) Definicja ?rodowiska MDP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "states = ['Healthy', 'Stable', 'Critical']\n",
        "actions = ['NoTreatment', 'Standard', 'Aggressive']\n",
        "state_idx = {s:i for i,s in enumerate(states)}\n",
        "action_idx = {a:i for i,a in enumerate(actions)}\n",
        "gamma = 0.95\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c) Definicja funkcji przej?cia i nagrody"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "state_value = {'Healthy': 2, 'Stable': 1, 'Critical': 0}\n",
        "costs = {'NoTreatment': 0.0, 'Standard': 1.0, 'Aggressive': 3.0}\n",
        "transition_probs = {\n",
        "    'Healthy': {\n",
        "        'NoTreatment': [('Healthy', 0.85), ('Stable', 0.15)],\n",
        "        'Standard': [('Healthy', 0.9), ('Stable', 0.1)],\n",
        "        'Aggressive': [('Healthy', 0.95), ('Stable', 0.05)]\n",
        "    },\n",
        "    'Stable': {\n",
        "        'NoTreatment': [('Stable', 0.6), ('Critical', 0.4)],\n",
        "        'Standard': [('Healthy', 0.4), ('Stable', 0.5), ('Critical', 0.1)],\n",
        "        'Aggressive': [('Healthy', 0.6), ('Stable', 0.35), ('Critical', 0.05)]\n",
        "    },\n",
        "    'Critical': {\n",
        "        'NoTreatment': [('Critical', 0.85), ('Stable', 0.15)],\n",
        "        'Standard': [('Critical', 0.6), ('Stable', 0.35), ('Healthy', 0.05)],\n",
        "        'Aggressive': [('Critical', 0.4), ('Stable', 0.5), ('Healthy', 0.1)]\n",
        "    }\n",
        "}\n",
        "\n",
        "def step(state, action, cost_override=None):\n",
        "    probs = transition_probs[state][action]\n",
        "    r = random.random()\n",
        "    cum = 0.0\n",
        "    next_state = probs[-1][0]\n",
        "    for s, p in probs:\n",
        "        cum += p\n",
        "        if r <= cum:\n",
        "            next_state = s\n",
        "            break\n",
        "    cost = costs[action] if cost_override is None else cost_override\n",
        "    reward = (state_value[next_state] - state_value[state]) * 10.0 - cost\n",
        "    return next_state, reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "d) Implementacja tablicowego Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Q = np.zeros((len(states), len(actions)))\n",
        "alpha = 0.2\n",
        "epsilon = 1.0\n",
        "epsilon_min = 0.05\n",
        "epsilon_decay = 0.995\n",
        "\n",
        "def select_action(state, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)\n",
        "    return actions[int(np.argmax(Q[state_idx[state]]))]\n",
        "\n",
        "def update_q(state, action, reward, next_state):\n",
        "    s = state_idx[state]\n",
        "    a = action_idx[action]\n",
        "    ns = state_idx[next_state]\n",
        "    target = reward + gamma * np.max(Q[ns])\n",
        "    Q[s, a] = Q[s, a] + alpha * (target - Q[s, a])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "e) Trening agenta Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "episodes = 500\n",
        "max_steps = 30\n",
        "q_rewards = []\n",
        "\n",
        "for ep in range(episodes):\n",
        "    state = random.choice(states)\n",
        "    total = 0.0\n",
        "    for t in range(max_steps):\n",
        "        action = select_action(state, epsilon)\n",
        "        next_state, reward = step(state, action)\n",
        "        update_q(state, action, reward, next_state)\n",
        "        state = next_state\n",
        "        total += reward\n",
        "    q_rewards.append(total)\n",
        "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "f) Analiza polityki i krzywej uczenia"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(q_rewards, label='Q-learning')\n",
        "plt.xlabel('Epizod')\n",
        "plt.ylabel('Suma nagr?d')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "g) Implementacja Deep Q-Network (DQN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_model():\n",
        "    model = keras.Sequential([\n",
        "        layers.Input(shape=(len(states),)),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(32, activation='relu'),\n",
        "        layers.Dense(len(actions), activation='linear')\n",
        "    ])\n",
        "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001), loss='mse')\n",
        "    return model\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=5000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "    def add(self, s, a, r, ns, done):\n",
        "        self.buffer.append((s, a, r, ns, done))\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        s, a, r, ns, done = map(np.array, zip(*batch))\n",
        "        return s, a, r, ns, done\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "model = build_model()\n",
        "target_model = build_model()\n",
        "target_model.set_weights(model.get_weights())\n",
        "\n",
        "buffer = ReplayBuffer()\n",
        "batch_size = 32\n",
        "dqn_gamma = 0.95\n",
        "dqn_epsilon = 1.0\n",
        "dqn_epsilon_min = 0.05\n",
        "dqn_epsilon_decay = 0.995\n",
        "update_target_every = 10\n",
        "\n",
        "def state_onehot(state):\n",
        "    v = np.zeros(len(states))\n",
        "    v[state_idx[state]] = 1.0\n",
        "    return v\n",
        "\n",
        "def dqn_action(state, epsilon):\n",
        "    if random.random() < epsilon:\n",
        "        return random.choice(actions)\n",
        "    q_vals = model.predict(state_onehot(state)[None, :], verbose=0)[0]\n",
        "    return actions[int(np.argmax(q_vals))]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "h) Trening DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dqn_episodes = 400\n",
        "max_steps = 30\n",
        "dqn_rewards = []\n",
        "\n",
        "for ep in range(dqn_episodes):\n",
        "    state = random.choice(states)\n",
        "    total = 0.0\n",
        "    for t in range(max_steps):\n",
        "        action = dqn_action(state, dqn_epsilon)\n",
        "        next_state, reward = step(state, action)\n",
        "        buffer.add(state_onehot(state), action_idx[action], reward, state_onehot(next_state), False)\n",
        "        state = next_state\n",
        "        total += reward\n",
        "        if len(buffer) >= batch_size:\n",
        "            s, a, r, ns, done = buffer.sample(batch_size)\n",
        "            q_next = target_model.predict(ns, verbose=0)\n",
        "            target = model.predict(s, verbose=0)\n",
        "            for i in range(batch_size):\n",
        "                target[i, a[i]] = r[i] + dqn_gamma * np.max(q_next[i])\n",
        "            model.fit(s, target, verbose=0)\n",
        "    dqn_rewards.append(total)\n",
        "    dqn_epsilon = max(dqn_epsilon_min, dqn_epsilon * dqn_epsilon_decay)\n",
        "    if (ep + 1) % update_target_every == 0:\n",
        "        target_model.set_weights(model.get_weights())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "i) Por?wnanie Q-learning vs DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8,4))\n",
        "plt.plot(q_rewards, label='Q-learning')\n",
        "plt.plot(np.linspace(0, len(q_rewards)-1, len(dqn_rewards)), dqn_rewards, label='DQN')\n",
        "plt.xlabel('Epizod')\n",
        "plt.ylabel('Suma nagr?d')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "j) Explainable Reinforcement Learning ? analiza Q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "state_to_explain = 'Stable'\n",
        "q_values = Q[state_idx[state_to_explain]]\n",
        "best_action_idx = int(np.argmax(q_values))\n",
        "best_action = actions[best_action_idx]\n",
        "print(state_to_explain, q_values, best_action)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "k) Wyja?nienie kontrfaktyczne decyzji"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "alt_action_idx = int(np.argsort(q_values)[-2])\n",
        "alt_action = actions[alt_action_idx]\n",
        "q_best = q_values[best_action_idx]\n",
        "q_alt = q_values[alt_action_idx]\n",
        "print(best_action, alt_action, q_best - q_alt)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "l) Analiza wra?liwo?ci na funkcj? nagrody"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_q_with_aggressive_cost(cost_aggr, episodes=300):\n",
        "    Q_local = np.zeros((len(states), len(actions)))\n",
        "    eps = 1.0\n",
        "    for ep in range(episodes):\n",
        "        state = random.choice(states)\n",
        "        for t in range(25):\n",
        "            if random.random() < eps:\n",
        "                action = random.choice(actions)\n",
        "            else:\n",
        "                action = actions[int(np.argmax(Q_local[state_idx[state]]))]\n",
        "            cost_override = cost_aggr if action == 'Aggressive' else costs[action]\n",
        "            next_state, reward = step(state, action, cost_override=cost_override)\n",
        "            s = state_idx[state]\n",
        "            a = action_idx[action]\n",
        "            ns = state_idx[next_state]\n",
        "            target = reward + gamma * np.max(Q_local[ns])\n",
        "            Q_local[s, a] = Q_local[s, a] + 0.2 * (target - Q_local[s, a])\n",
        "            state = next_state\n",
        "        eps = max(0.05, eps * 0.99)\n",
        "    policy = {s: actions[int(np.argmax(Q_local[state_idx[s]]))] for s in states}\n",
        "    return policy\n",
        "\n",
        "policy_default = {s: actions[int(np.argmax(Q[state_idx[s]]))] for s in states}\n",
        "policy_high_cost = train_q_with_aggressive_cost(6.0)\n",
        "print(policy_default)\n",
        "print(policy_high_cost)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
